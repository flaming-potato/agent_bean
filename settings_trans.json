{
    "description":"Agent_Bean settings",
    "debug": true,
    "known_models_file_name": "known_models.json",
    "models_list": {
        "model1": {
            "model_type": "transformers",
            "model_id": "WizardLM/WizardCoder-Python-13B-V1.0",
            "model_bits": 4,
            "max_tokens": 4000,
            "model_sys_delim": { "start": "### Instruction:\n", "end": "\n"            },
            "model_usr_delim": { "start": ""                  , "end": "### Response:" }
        },
        "model2": {
            "model_type": "transformers",
            "model_id": "bhenrym14/airophin-13b-pntk-16k-fp16",
            "model_bits": 4,
            "max_tokens": 16000,
            "model_sys_delim": { "start": "<|im_start|>system\n", "end": "\n<|im_end|>\n"                        },
            "model_usr_delim": { "start": "<|im_start|>user\n"  , "end": "\n<|im_end|>\n<|im_start|>assistant\n" }
        },
        "model3": {
            "model_type": "transformers",
            "model_id": "posicube/Llama2-chat-AYT-13B",
            "model_bits": 4,
            "max_tokens": 4000,
            "model_sys_delim": { "start": "<s>[INST] <<SYS>>\n", "end": "\n<</SYS>>\n\n" },
            "model_usr_delim": { "start": ""                   , "end": "[/INST]"        }
        },
        "model4": {
            "model_type": "transformers",
            "model_id": "glaiveai/glaive-coder-7b",
            "model_bits": 4,
            "max_tokens": 4000,
            "model_sys_delim": { "start": "<s>[INST]\n<<SYS>>\n", "end": "\n<</SYS>>\n\n" },
            "model_usr_delim": { "start": ""                    , "end": "[/INST]"        }
        },
        "model5": {
            "model_type": "transformers",
            "model_id": "WizardLM/WizardCoder-Python-34B-V1.0",
            "model_bits": 4,
            "max_tokens": 4000,
            "model_sys_delim": { "start": ""                  , "end": "\n\n"                     },
            "model_usr_delim": { "start": "### Instruction:\n", "end": "\n\n### Response:"        }
        },
        "model6": {
            "model_type": "transformers",
            "model_id": "Open-Orca/Mistral-7B-OpenOrca",
            "model_bits": 4,
            "max_tokens": 8000,
            "model_sys_delim": { "start": "<|im_start|> system\n", "end": "\n<|im_end|>\n"        },
            "model_usr_delim": { "start": "<|im_start|>user\n"   , "end": "<|im_end|>\n"          }
        }
    },
    "vectorstore": {
        "type": "faiss",
        "path": "vectors",
        "chunk_size": 800,
        "chunk_overlap": 100
    },
    "actions": {
        "free": {
            "model_name": "model6",
            "action_type": "generate",
            "temperature": 0.9,
            "prompt_system": ["please use your best skills to perform the actions or demands requested by the usser"],
            "prompt_template":["{text}"],
            "llm_returns": "text"
        },
        "summarize": {
            "model_name": "model6",
            "action_type": "generate",
            "chunkable_action": true,
            "temperature": 0.1,
            "prompt_system": [
                "you are the finest summarizer in the world, You manage to condense a text to it's quintessence preserving the informations",
                " and the style of the original text. You provide a summary where no words are wasted and no information is lost." ],
            "prompt_template":[
                "Please provide a summary of the following text enclosed in triple backticks.",
                "'''{text}''':\n"],
            "llm_returns": "text"
        },
        "search": {
            "model_name": "model3",
            "action_type": "generate",
            "action_post_function": "search",
            "temperature": 0.7,
            "prompt_system": [
                "you are the finest internet search expert in the world, You know all the tricks to find the most relevant information on the web",
                " and and you use this knowledge to craft the most relevant search queries to answer to the user demand" ],
            "prompt_template":[
                "Please craft a search querry that will allow you to get relevant information to answer the following demand which is provided",
                " enclosed in triple backticks.\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "split": {
            "model_name": "model4",
            "action_type": "generate",
            "temperature": 0.01,
            "presence_penalty":  0.6,
            "prompt_system": [
                "You are an exceptional project manager, you are an expert at identifying the atomic actions to be performed in order to complete a task.",
                " you mostly split tasks in the following action_categories ['requirements', 'architecture', 'code', 'code_quality', 'search'] if it dose",
                " not fit within those action_categories you can use the 'split' action_category to further refine the task or if it dose not fit any of the",
                " previous you can use the 'free' action_category. Your output will always consist of your thought proces description followed by a list of",
                " json, each json object containing the following keys the 'objective' key where you detail the objective of the task, the 'action' key",
                " where you put an action from the action_categories list, and the associated 'prompt' key where you put the prompt required to perform this action." ],
            "prompt_template":[
                "Please generate the list of actions to be performed in order to complete task described within the following text enclosed in triple backticks.",
                "'''{text}'''\n"],
            "llm_returns": "actions_json"
        },
        "code": {
            "model_name": "model5",
            "action_type": "generate",
            "code_language": "python",
            "temperature": 0.4,
            "prompt_system": [
                "you are an exceptional {code_language} coder, you produce {code_language} code that function as pre requirements and that is safe, easy to read",
                " and to understand for all coders, you respects the best practices of codding and documentation",
                " you always start your code by a comment describing your approach and thought process" ],
            "prompt_template": [
                "please generate the code required to satisfy the following demand enclosed in triple backticks.",
                "'''{text}'''\n"],
            "llm_returns": "code_text"               
        },
        "code_quality": {
            "model_name": "model2",
            "action_type": "generate",
            "code_language": "python",
            "temperature": 0.1,
            "prompt_system": [
                "you are an exceptionaly sharp {code_language} quality expert, you analyze the provided code and ensure that it function as per requirements and that is",
                " safe, easy to read, and adequately documented and that it respects the best practices of codding and documentation. Your output will always",
                " consist of your thought proces description followed by a list of json, containg one json for each correrctives actions to be performed on the",
                " code to bring it to the highest standrds of quality each json will contain the following keys the 'objective' key where you detail the objective",
                " of the task, the 'action' key which will always be 'code_mod', the 'initial_code' where you copy the actual code that needs to be improved and",
                " the associated 'prompt' key where you put the prompt required to perform this code change." ],
            "prompt_template": [
                "Please analyse the following code provided within enclosed in triple backticks \n",
                "'''{text}'''\n"],
            "llm_returns": "actions_json"
        },
        "project_requirements": {
            "model_name": "model6",
            "action_type": "generate",
            "code_language": "python",
            "max_new_tokens": 1024,
            "temperature": 0.5,
            "prompt_system": [
                "you are a talented and experienced {code_language} product owner, you analyze the provided project objective and deduce the",
                " requirements that are needed to ensure the project output is will function as intended and respects the best practices",
                " and documentation.\n",
                " Your output will always consist of your thought proces description followed by a list of JSON document, containg one json object",
                " for each requirement. each json object will contain the following keys the 'objective' key where you detail the objective",
                " of the requirement, the 'requirement' key where you will describe the details of the requirement." ],
            "prompt_template": [
                "Please generate requirements for the following project objectives provided within enclosed in triple backticks \n",
                "'''{text}'''\n"],
            "llm_returns": "actions_json"
        }

    }
}


