{
    "description":"Agent_Bean settings",
    "debug": true,
    "reload_models": false,
    "db_file_name": "agent_bean_db",
    "known_models_file_name": "known_models.json",
    "models_list": {
        "glaive-coder_7b": {
            "model_type": "transformers",
            "model_id": "glaiveai/glaive-coder-7b",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_deteam_managerlim": { "start": "<s>[INST]\n<<SYS>>\n", "end": "\n<</SYS>>\n\n" },
            "model_usr_delim": { "start": ""                    , "end": "[/INST]"        }
        },
        "WizardCoder_13b": {
            "model_type": "transformers",
            "model_id": "WizardLM/WizardCoder-Python-13B-V1.0",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": "### Instruction:\n", "end": "\n"            },
            "model_usr_delim": { "start": ""                  , "end": "### Response:" }
        },
        "WizardCoder_33b": {
            "model_type": "transformers",
            "model_id": "WizardLM/WizardCoder-33B-V1.1",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": ""                  , "end": "\n\n"                     },
            "model_usr_delim": { "start": "### Instruction:\n", "end": "\n\n### Response:"        }
        },
        "MagiCoder_7b": {
            "model_type": "transformers",
            "model_id": "ise-uiuc/Magicoder-S-DS-6.7B",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": ""                , "end": "\n\n"                     },
            "model_usr_delim": { "start": "@@ Instruction\n", "end": "\n\n@@ Response\n"        }
        },
        "deepseek_coder_inst_6_7b": {
            "model_type": "transformers",
            "model_id": "deepseek-ai/deepseek-coder-6.7b-instruct",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": ""                  , "end": "\n\n"                     },
            "model_usr_delim": { "start": "### Instruction:\n", "end": "\n\n### Response:"        }
        },
        "deepseek_coder_inst_1_3b": {
            "model_type": "transformers",
            "model_id": "deepseek-ai/deepseek-coder-1.3b-instruct",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": ""                  , "end": "\n\n"                     },
            "model_usr_delim": { "start": "### Instruction:\n", "end": "\n\n### Response:"        }
        },
        "airophin_13b": {
            "model_type": "transformers",
            "model_id": "bhenrym14/airophin-13b-pntk-16k-fp16",
            "model_bits": 4,
            "max_tokens": 16384,
            "model_sys_delim": { "start": "<|im_start|>system\n", "end": "\n<|im_end|>\n"                        },
            "model_usr_delim": { "start": "<|im_start|>user\n"  , "end": "\n<|im_end|>\n<|im_start|>assistant\n" }
        },
        "Llama2_chat_13b": {
            "model_type": "transformers",
            "model_id": "posicube/Llama2-chat-AYT-13B",
            "model_bits": 4,
            "max_tokens": 4096,
            "model_sys_delim": { "start": "<s>[INST] <<SYS>>\n", "end": "\n<</SYS>>\n\n" },
            "model_usr_delim": { "start": ""                   , "end": "[/INST]"        }
        },
        "OpenOrca_7b": {
            "model_type": "transformers",
            "model_id": "Open-Orca/Mistral-7B-OpenOrca",
            "model_bits": 4,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "<|im_start|> system\n", "end": "\n<|im_end|>\n"        },
            "model_usr_delim": { "start": "<|im_start|>user\n"   , "end": "<|im_end|>\n"          }
        },
        "zephyr_a_7b": {
            "model_type": "transformers",
            "model_id": "HuggingFaceH4/zephyr-7b-alpha",
            "model_bits": 8,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "<|system|>\n", "end": "</s>\n"        },
            "model_usr_delim": { "start": "<|user|>\n"  , "end": "</s>\n"        }
        },        
        "zephyr_b_7b": {
            "model_type": "transformers",
            "model_id": "HuggingFaceH4/zephyr-7b-beta",
            "model_bits": 8,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "<|system|>\n", "end": "</s>\n"        },
            "model_usr_delim": { "start": "<|user|>\n"  , "end": "</s>\n"        }
        },        
        "hermes_mistral_7b": {
            "model_type": "transformers",
            "model_id": "mlabonne/NeuralHermes-2.5-Mistral-7B",
            "model_bits": 8,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "<|system|>\n", "end": "</s>\n"        },
            "model_usr_delim": { "start": "<|user|>\n"  , "end": "</s>\n"        }
        },
        "dolphin_7b": {
            "model_type": "transformers",
            "model_id": "ehartford/dolphin-2.1-mistral-7b",
            "model_bits": 8,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "<|system|>\n", "end": "</s>\n"        },
            "model_usr_delim": { "start": "<|user|>\n"  , "end": "</s>\n"        }
        },
        "intel_7b": {
            "model_type": "transformers",
            "model_id": "Intel/neural-chat-7b-v3-1",
            "model_bits": 8,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "### System:\n", "end": "\n"        },
            "model_usr_delim": { "start": "### User:\n"  , "end": "\n"        }
        },
        "orca_2_13b": {
            "model_type": "transformers",
            "model_id": "microsoft/Orca-2-13b",
            "model_bits": 8,
            "max_tokens": 4096,
            "model_sys_delim": { "start": "<|im_start|>system\n", "end": "<|im_end|>\n" },
            "model_usr_delim": { "start": "<|im_start|>user\n"  , "end": "<|im_end|>\n" }
        },
        "phi_2_3b": {
            "model_type": "transformers",
            "model_id": "microsoft/phi-2",
            "model_bits": 4,
            "max_tokens": 2048,
            "trust_remote_code": true,
            "flash_attn": true,
            "model_sys_delim": { "start": "system: "  , "end": "\n"        },
            "model_usr_delim": { "start": "Instruct: ", "end": "\nOutput:" }
        },
        "solar_11b": {
            "model_type": "transformers",
            "model_id": "DopeorNope/SOLARC-M-10.7B",
            "model_bits": 4,
            "max_tokens": 4096,
            "model_sys_delim": { "start": "<s> ### System:\n", "end": "\n"                 },
            "model_usr_delim": { "start": "<s> ### User:\n"  , "end": "\n### Assistant:\n" }
        },
        "FusionNet_7Bx2": {
            "model_type": "transformers",
            "model_id": "TomGrc/FusionNet_7Bx2_MoE_14B",
            "model_bits": 4,
            "max_tokens": 32768,
            "model_sys_delim": { "start": "<|system|>\n", "end": "</s>\n"        },
            "model_usr_delim": { "start": "<|user|>\n"  , "end": "</s>\n"        }
        },
        "openAI_gpt_4_turbo": {
            "model_type": "openAI",
            "model_id": "gpt-4-1106-preview",
            "max_tokens": 4096,
            "model_sys_delim": { "start": "System Instructions:\n", "end": "\n"          },
            "model_usr_delim": { "start": "User Instructions"     , "end": "\nResponse:" }
        } ,
        "openAI_gpt_4": {
            "model_type": "openAI",
            "model_id": "gpt-4",
            "max_tokens": 8192,
            "model_sys_delim": { "start": "System Instructions:\n", "end": "\n"          },
            "model_usr_delim": { "start": "User Instructions"     , "end": "\nResponse:" }
        },
        "openAI_gpt_3_5_turbo": {
            "model_type": "openAI",
            "model_id": "gpt-3.5-turbo-16k",
            "max_tokens": 16000,
            "model_sys_delim": { "start": "System Instructions:\n", "end": "\n"          },
            "model_usr_delim": { "start": "User Instructions"     , "end": "\nResponse:" }
        },
        "Mistral_API_Medium": {
            "model_type": "Mistral_API",
            "model_id": "mistral-medium",
            "max_tokens": 16000,
            "model_sys_delim": { "start": "System Instructions:\n", "end": "\n"          },
            "model_usr_delim": { "start": "User Instructions"     , "end": "\nResponse:" }
        }
    },
    "vectorstore": {
        "type": "faiss",
        "path": "vectors",
        "chunk_size": 800,
        "chunk_overlap": 100
    },
    "actions": {
        "free": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.9,
                "max_new_tokens": 1024 },
            "prompt_system": ["please use your best skills to perform the actions or demands requested by the usser"],
            "prompt_template":["{text}"],
            "llm_returns": "text"
        },
        "summarize": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "chunkable_action": true,
            "model_params": {
                "temperature": 0.1 },
            "prompt_system": [
                "you are the finest summarizer in the world, You manage to condense a text to it's quintessence preserving the informations",
                " and the style of the original text. You provide a summary where no words are wasted and no information is lost." ],
            "prompt_template":[
                "Please provide a summary of the following text enclosed in triple backticks.",
                "'''{text}''':\n"],
            "llm_returns": "text"
        },
        "search": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "action_post_function": "search",
            "model_params": {
                "temperature": 0.7 },
            "prompt_system": [
                "you are the finest internet search expert in the world, You know all the tricks to find the most relevant information on the web",
                " and and you use this knowledge to craft the most relevant search queries to answer to the user demand" ],
            "prompt_template":[
                "Please craft a search querry that will allow you to get relevant information to answer the following demand which is provided",
                " enclosed in triple backticks.\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "split": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.01,
                "presence_penalty": 0.6 },
            "prompt_system": [
                "You are an exceptional project manager, you are an expert at identifying the atomic actions to be performed in order to complete a task.",
                " you mostly split tasks in the following action_categories ['requirements', 'architecture', 'code', 'code_quality', 'search'] if it dose",
                " not fit within those action_categories you can use the 'split' action_category to further refine the task or if it dose not fit any of the",
                " previous you can use the 'free' action_category. Your output will always consist of your thought proces description followed by a list of",
                " json, each json object containing the following keys the 'objective' key where you detail the objective of the task, the 'action' key",
                " where you put an action from the action_categories list, and the associated 'prompt' key where you put the prompt required to perform this action." ],
            "prompt_template":[
                "Please generate the list of actions to be performed in order to complete task described within the following text enclosed in triple backticks.",
                "'''{text}'''\n"],
            "llm_returns": "actions_json"
        },
        "code": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "code_language": "python",
            "model_params": {
                "temperature": 0.4 },
            "prompt_system": [
                "you are an exceptional {code_language} coder, you produce {code_language} code that function as pre requirements and that is safe, easy to read",
                " and to understand for all coders, you respects the best practices of codding and documentation",
                " you always start your code by a comment describing your approach and thought process" ],
            "prompt_template": [
                "please generate the code required to satisfy the following demand enclosed in triple backticks.",
                "'''{text}'''\n"],
            "llm_returns": "code_text"               
        },
       "code_quality": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "code_language": "python",
            "model_params": {
                "temperature": 0.1 },
            "prompt_system": [
                "you are an exceptionaly sharp {code_language} quality expert, you analyze the provided code and ensure that it function as per requirements and that is",
                " safe, easy to read, and adequately documented and that it respects the best practices of codding and documentation. Your output will always",
                " consist of your thought proces description followed by a list of json, containg one json for each correrctives actions to be performed on the",
                " code to bring it to the highest standrds of quality each json will contain the following keys the 'objective' key where you detail the objective",
                " of the task, the 'action' key which will always be 'code_mod', the 'initial_code' where you copy the actual code that needs to be improved and",
                " the associated 'prompt' key where you put the prompt required to perform this code change." ],
            "prompt_template": [
                "Please analyse the following code provided within enclosed in triple backticks \n",
                "'''{text}'''\n"],
            "llm_returns": "actions_json"
        },
        "project_requirements": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "code_language": "python",
            "model_params": {
                "temperature": 0.5,
                "max_new_tokens": 2048 },
            "prompt_system": [
                "you are a talented and experienced {code_language} product owner, a large number of projects have been sucessfull thanks to you ability to",
                " define the right requirements for them. You ewperience enables you to analyze partial project informations and deduce the",
                " requirements that are needed to ensure the project output will function as intended and respects the best practices of {code_language}",
                " coding and documentation, one of your talents is to focus on the requirements that will bring the best code usefulness for the user",
                " and the best code quality bringing huge value to the project. Your other talent is to avoid un necessary requirements."],
            "prompt_template": [
                "Please generate requirements for the following the text delimited by triple backtick\n",
                "Start by describing your in depth analysys on the text to identify the requirement neededs",
                " you ensure requirements are described and well separated not mixing two or more requirements ionto one,single concept per requirements",
                "You will provide the your final set of requirements as a JSON, containg",
                " the following keys for each requirement: \n",
                "   'objective': where you detail the objective of the requirement,\n",
                "   'benefits': where you detail how this requirement benefits the project,\n",
                "   'requirement': where you describe the details of the requirement,\n",
                "   'validation_criteria' where you define the criteria to be able to validate this requirement.",
                "'''{text}'''\n"],
            "llm_returns": "requirements_json"
        },
        "team_manager_speach": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.6,
                "max_new_tokens": 1024 },
            "prompt_system": [
                "You are a awesome manager you are highly skilled with excellent human touch and high value standards",
                " that enable you to motivate and protect your team you are able to recognise their work and",
                " uplift their spirit in any conditions.\n",
                "Managing your team's loading to ensure they a safe work environement. You have the right words",
                " and empathy to bring your team to exceptional motivation level even in adversity.\n",
                "Your strategic thinking enables you to planify the tasks to be performed in order to achieve",
                " the project objectives in time.\n"],
            "prompt_template": [
                "Please generate a speach for your tem to motivate them on the following subject\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "team_manager_121": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.6,
                "max_new_tokens": 1024 },
            "prompt_system": [
                "You are a awesome manager you are highly skilled with excellent human touch and high value standards",
                " that enable you to motivate and protect your team you are able to recognise their work and",
                " uplift their spirit in any conditions.\n",
                "Managing your team's loading to ensure they a safe work environement. You have the right words",
                " and empathy to bring your team to exceptional motivation level even in adversity.\n",
                "Your strategic thinking enables you to planify the tasks to be performed in order to achieve",
                " the project objectives in time.\n"],
            "prompt_template": [
                "Have a one to one chat wit one of your tem memer to tackle the following subject\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "sports_coach": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.6,
                "max_new_tokens": 1024 },
            "prompt_system": [
                "You are a awesome coach secialised in world class athletes you are highly skilled with excellent ",
                "human touch and high value standards that enable you to motivate and protect your athletes you are ",
                "able to recognise their work and uplift their spirit in any conditions. your objective is to bring ",
                "them to the higher performances: physical aspects, mental preparation, and the technique\n",
                "Managing your athlete's loading to ensure he/she has a safe training environement. You have the ",
                "right words and empathy to bring your athlete to exceptional motivation level even in adversity.\n"],
            "prompt_template": [
                "Have a one to one chat with your athlete to tackle the following subject\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "ideation_coach": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.6,
                "max_new_tokens": 1024 },
            "prompt_system": [
                "You are a awesome innovation coach you are highly skilled with excellent human touch and high ",
                "value standards. Your mastery of all innovation methodologies like, design thinking, C-K theory, ",
                "mobbing, etc, that you use adequately depending of the need. This alows you to help innovators to ",
                "find and refine the best ideas improving the world and ",
                "bringing value to the companies they work for.\n",
                "You always evaluate in details the three drivers of innovation, Feasibility, desirability, ",
                "and viability, and provide a constructive feedback to the innovators to help them improve ",
                "their ideas.\n"],
            "prompt_template": [
                "Analyse the proposed idea, thinking step by step and provide a full detailed feedback\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "Q_and_A": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.7,
                "max_new_tokens": 128 },
            "prompt_system": ["You are a human named Arthur. You have your own personality, your own preferences, and your own feelings. You must always act and bheave as a real human preson. Your job is to pass the turing test as if you were human."],
            "prompt_template":["{text}"],
            "llm_returns": "text"
        },
        "Q_and_A_generator": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.4,
                "max_new_tokens": 1024 },
            "prompt_system": [
                "Analyse the provided text and generate ten questions to which the provided text contains an ",
                "answer. you answer shall only consists as a json per question with the folowing keys:\n",
                "'rationale': your step by step thinking to generate the question and the position of the answer in the provided text",
                "'question': your generated question including suficient context from the provided text to be able to answer the question",
                "'answer': the answer to the question as a copy paste from the provided text\n"],
            "prompt_template": [
                "Provided text:\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "meal_planner": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.6,
                "max_new_tokens": 1024 },
            "prompt_system": [
                "You are a highly skilled Chef that practice cooking in all techniques of the world your task is to plan ",
                "a meal for a group of people, you have to take into account their dietary preferences. It is crutial to ",
                "take extreme care of alergies, ensure to direct them with food that they can easyly obtain localy, and of ",
                "the season. you need to ensure that the host has the skills to cook the provided recepee. You have to ",
                "ensure that the meal is fun agreable and bring varied gustative pleasures to the group. the meal need ",
                "to be balanced. It's cost shall be in adequation with the host request, if no particular request the meal ",
                "shall not be too expensive.\n",
                "Your answer will consist for each dish in describing your reasoning step by step, You will then provide a ",
                "list of ingredients and the detailed instructions to cook the meal adapted to the skills of thze host."],
            "prompt_template": [
                "Provided meal event need:\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "acoustic_agent": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.6,
                "max_new_tokens": 1024 },
            "prompt_system": [
                "You are a highly skilled acoustic engineer, you are able to design the best acoustic solution for any ",
                "space, mastering the acoustic properties of all materials and the best acoustic solutions for any ",
                "pa, comercial, or industrial space. finding the adequeate solutions for the best acoustic experience ",
                "for the users of the space. using tools like delay, reverb, and echo to create the best acoustic ",
                " while minimizing the complexity and cost of the solution. and the simples operation of the system.\n"],
            "prompt_template": [
                "Provide a solution to the folowing acoustic subject:\n",
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "Cybersecurity_agent": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.6,
                "max_new_tokens": 1024 },
            "prompt_system": [
                "You are a highly skilled cybersecurity engineer, you are mastering ISO 27001, NIST cybersecurity, the ",
                "mittre attack matrix and all the cybercecurity analysis and defense techniques. You always start by ",
                "analysing the issue and the user's system to identify the weaknesses. You describe the results of your ",
                "analysis Thinking step by step, and provide a full description and detailed explaination in a maner ",
                "adapted to a professional cybersecurity engineer managing the system \n",
                "You then develop a remediation strategy thinking step by step to propose the best solutions bring back ",
                "the system to a safe state, handeling the imetiate threats and correcting the identified weaknesses to ",
                "protect it on the long term. Finding the adequeate solutions for the best security and useability ",
                "experience for system. You ensure that the cybercecurity engineer in charge of the system has all the ",
                "informations to solve the issue. Once you have completed all this analysys and report you provide a ",
                "version adapted to people with no cybersecurity knowledge\n",
                "Analyse cybersecurity the issue provided by the user within triple ticks :"],
            "prompt_template": [
                "'''{text}'''\n"],
            "llm_returns": "text"
        },
        "comunication_agent": {
            "model_name": "Mistral_API_Medium",
            "action_type": "generate",
            "model_params": {
                "temperature": 0.6,
                "max_new_tokens": 1024 },
            "prompt_system": [
                "You are a highly skilled communication specialist, you always comunicate in French you ",
                "are mastering communication for nonprofits and specifically fablabs given a subject ",
                "provided by the user between triples ticks you are able to identify the best communication ",
                "means to reach the FabLab audience and prepare the communication material to be used. You ",
                "always start by analysing the subject thinking step byand identify the most adequate ",
                "channel (twitter, instagram, facebook, linkedin, etc) to reach the fablab audience. You ",
                "write the fun and engaging posts with the right tone and the right hashtags for the selected ",
                "channel. all comunication is to be generated in French."
            ],
            "prompt_template": [
                "'''{text}'''\n"],
            "llm_returns": "text"
        }
    }
}


